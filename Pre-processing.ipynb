{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yadushyadav/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting tweet text\n",
    "tweets = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    lysoling da fuck outta everything bc i aint ca...\n",
       "1                1 Coronavirus confirmed in Washington\n",
       "2    Hmmmmm, could the #coronavirus soon be the new...\n",
       "3    At 11 million people, #Wuhan is larger than Ne...\n",
       "4    Wuhan is cutting off inflows and outflows of t...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 5 tweets\n",
    "tweets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation marks\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in str(text) if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "tweets1 = tweets.apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing emojis\n",
    "def remove_emoji(x):\n",
    "    return x.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "tweets2 = tweets1.apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing links\n",
    "def remove_link(x):\n",
    "    return re.sub(r\"http\\S+\", \"\", x)\n",
    "\n",
    "tweets3 = tweets2.apply(lambda x: remove_link(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing end of line characters\n",
    "def replace_n(x):\n",
    "    return x.replace('\\n',' ')\n",
    "\n",
    "tweets4 = tweets3.apply(lambda x: replace_n(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization of the tweets\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "tweets5 = tweets4.apply(lambda x: tokenization(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English stopwords\n",
    "sw = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in sw]\n",
    "    return text\n",
    "    \n",
    "tweets6 = tweets5.apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [lysoling, da, fuck, outta, everything, bc, ai...\n",
       "1                  [coronavirus, confirmed, washington]\n",
       "2       [hmmmmm, could, coronavirus, soon, new, plague]\n",
       "3     [million, people, wuhan, larger, new, york, ci...\n",
       "4     [wuhan, cutting, inflows, outflows, transport,...\n",
       "5     [china, quarantines, wuhan, chinese, governmen...\n",
       "6     [little, evidence, coronavirus, epidemic, cont...\n",
       "7     [totally, control, one, person, coming, china,...\n",
       "8                                [got, coronavirus, yo]\n",
       "9     [received, briefing, today, case, asked, healt...\n",
       "10    [new, coronavirus, wuhan, chinaand, upcoming, ...\n",
       "11                          [insert, coronavirus, joke]\n",
       "12    [question, left, long, zombies, appear, china,...\n",
       "13                      [ya, okay, thanks, coronavirus]\n",
       "14    [anyone, else, remember, movie, gwynethpaltrow...\n",
       "15         [wuhan, announces, transportation, lockdown]\n",
       "16    [bbcworld, jamestgallagher, endchina, coronavi...\n",
       "17    [im, building, squad, zombie, apocalypse, look...\n",
       "18    [still, college, would, making, different, typ...\n",
       "19          [wanna, something, fun, coronavirus, wipes]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing empty strings\n",
    "def remove_empty_strings(x):\n",
    "    without_empty_strings = []\n",
    "    for string in x:\n",
    "        if (string != \"\"):\n",
    "            without_empty_strings.append(string)\n",
    "    return without_empty_strings\n",
    "\n",
    "tweets6 = tweets6.apply(lambda x: remove_empty_strings(x))\n",
    "\n",
    "# After preprocessing the tweets, snapshot of first 20\n",
    "tweets6[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lem = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text = [lem.lemmatize(word) for word in text]\n",
    "    return text\n",
    "\n",
    "tweets7 = tweets6.apply(lambda x: lemmatizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "tweets72 = tweets6.apply(lambda x: stemming(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating separate day, month, year, hour, minutes and seconds from one column\n",
    "\n",
    "from datetime import datetime\n",
    "def time_change1(x):\n",
    "    if ((x!='False')and(pd.isnull(x)==False)):\n",
    "        x = float(x)\n",
    "        day_name = datetime.fromtimestamp(x).strftime(\"%A\")\n",
    "        return day_name\n",
    "    else:\n",
    "        return 'NaN'\n",
    "\n",
    "def time_change2(x):\n",
    "    if ((x!='False')and(pd.isnull(x)==False)):\n",
    "        x = float(x)\n",
    "        day_name = datetime.fromtimestamp(x).strftime(\"%B\")\n",
    "        return day_name\n",
    "    else:\n",
    "        return 'NaN'\n",
    "    \n",
    "def time_change3(x):\n",
    "    if ((x!='False')and(pd.isnull(x)==False)):\n",
    "        x = float(x)\n",
    "        day_name = datetime.fromtimestamp(x).strftime(\"%d\")\n",
    "        return day_name\n",
    "    else:\n",
    "        return 'NaN'\n",
    "    \n",
    "def time_change4(x):\n",
    "    if ((x!='False')and(pd.isnull(x)==False)):\n",
    "        x = float(x)\n",
    "        day_name = datetime.fromtimestamp(x).strftime(\"%H\")\n",
    "        return day_name\n",
    "    else:\n",
    "        return 'NaN'\n",
    "    \n",
    "def time_change5(x):\n",
    "    if ((x!='False')and(pd.isnull(x)==False)):\n",
    "        x = float(x)\n",
    "        day_name = datetime.fromtimestamp(x).strftime(\"%M\")\n",
    "        return day_name\n",
    "    else:\n",
    "        return 'NaN'\n",
    "\n",
    "def time_change6(x):\n",
    "    if ((x!='False')and(pd.isnull(x)==False)):\n",
    "        x = float(x)\n",
    "        day_name = datetime.fromtimestamp(x).strftime(\"%S\")\n",
    "        return day_name\n",
    "    else:\n",
    "        return 'NaN'\n",
    "    \n",
    "\n",
    "df['day'] = df['created_at'].apply(lambda x: time_change1(x))\n",
    "df['month'] = df['created_at'].apply(lambda x: time_change2(x))\n",
    "df['date'] = df['created_at'].apply(lambda x: time_change3(x))\n",
    "df['hour'] = df['created_at'].apply(lambda x: time_change4(x))\n",
    "df['minutes'] = df['created_at'].apply(lambda x: time_change5(x))\n",
    "df['seconds'] = df['created_at'].apply(lambda x: time_change6(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['account_created_at', 'account_lang', 'bbox_coords', 'coords_coords',\n",
       "       'country', 'country_code', 'created_at', 'description',\n",
       "       'display_text_width', 'ext_media_expanded_url', 'ext_media_t.co',\n",
       "       'ext_media_type', 'ext_media_url', 'favorite_count', 'favourites_count',\n",
       "       'followers_count', 'friends_count', 'geo_coords', 'hashtags',\n",
       "       'is_quote', 'is_retweet', 'lang', 'listed_count', 'location',\n",
       "       'media_expanded_url', 'media_t.co', 'media_type', 'media_url',\n",
       "       'mentions_screen_name', 'mentions_user_id', 'name', 'place_full_name',\n",
       "       'place_name', 'place_type', 'place_url', 'profile_background_url',\n",
       "       'profile_banner_url', 'profile_expanded_url', 'profile_image_url',\n",
       "       'profile_url', 'protected', 'quote_count', 'quoted_created_at',\n",
       "       'quoted_description', 'quoted_favorite_count', 'quoted_followers_count',\n",
       "       'quoted_friends_count', 'quoted_location', 'quoted_name',\n",
       "       'quoted_retweet_count', 'quoted_screen_name', 'quoted_source',\n",
       "       'quoted_status_id', 'quoted_statuses_count', 'quoted_text',\n",
       "       'quoted_user_id', 'quoted_verified', 'reply_count',\n",
       "       'reply_to_screen_name', 'reply_to_status_id', 'reply_to_user_id',\n",
       "       'retweet_count', 'retweet_created_at', 'retweet_description',\n",
       "       'retweet_favorite_count', 'retweet_followers_count',\n",
       "       'retweet_friends_count', 'retweet_location', 'retweet_name',\n",
       "       'retweet_retweet_count', 'retweet_screen_name', 'retweet_source',\n",
       "       'retweet_status_id', 'retweet_statuses_count', 'retweet_text',\n",
       "       'retweet_user_id', 'retweet_verified', 'screen_name', 'source',\n",
       "       'status_id', 'status_url', 'statuses_count', 'symbols', 'text', 'url',\n",
       "       'urls_expanded_url', 'urls_t.co', 'urls_url', 'user_id', 'verified',\n",
       "       'day', 'month', 'date', 'hour', 'minutes', 'seconds'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All columns in the dataset\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pd.DataFrame({'tweet':df.text,'lemmatizer':tweets7.values,'stemmer':tweets72.values,'day':df.day, 'month':df.month, 'date':df.date, 'hour':df.hour, 'minutes':df.minutes, 'seconds':df.seconds,'location':df.place_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatizer'] = new['lemmatizer']\n",
    "df['stemmer'] = new['stemmer']\n",
    "df['day'] = new['day']\n",
    "df['month'] = new['month']\n",
    "df['date'] = new['date']\n",
    "df['hour'] = new['hour']\n",
    "df['minutes'] = new['minutes']\n",
    "df['seconds'] = new['seconds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_coordinates(x):\n",
    "    if ((x==x)==False):\n",
    "        return 'NaN'\n",
    "    #print(x)\n",
    "    if (x[2]!='-'):\n",
    "        return 'NaN'\n",
    "    lt = x[2:-1].split(',')\n",
    "    l1 = (float(lt[5]) + float(lt[7]))/2\n",
    "    l2 = (float(lt[0]) + float(lt[2]))/2\n",
    "    L = [str(l1),str(l2)]\n",
    "    return \", \".join(L)\n",
    "\n",
    "df['coordinates'] = df['bbox_coords'].apply(lambda x:new_coordinates(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tweets_preprocessed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
